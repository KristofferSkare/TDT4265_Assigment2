{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "$$\n",
    "w_{ji} = w_{ji} - \\alpha \\frac {\\partial C} {\\partial w_{ji}} =  w_{ji} - \\alpha \\frac {\\partial C} {\\partial z_j} \\cdot \\frac {\\partial z_j } {\\partial w_{ji}} =  w_{ji} - \\alpha \\delta_j \\cdot x_i \n",
    "$$\n",
    "$$\n",
    "\\delta_k^i = \\frac{\\partial C} {\\partial z_k^i} = \\sum_k \\frac {\\partial C}{\\partial z_k^{i+1}} \\cdot \\frac{\\partial z_k^{i+1}}{\\partial z_j^i} = \\sum_k \\delta_k^{i+1} \\cdot f'(z_j^i) \\cdot w_{kj}^i = f'(z_j^i) \\cdot \\sum_k \\delta_k^{i+1} \\cdot  w_{kj}^i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "Defining:\n",
    "\n",
    "I: number of input nodes\n",
    "J: number of nodes in hidden layer\n",
    "K: number of output nodes\n",
    "\n",
    "$W_1 = [w_{ji}]$ (shape: $J \\times I$)\n",
    "\n",
    "$W_2 = [w_{kj}]$ (shape: $K \\times J$)\n",
    "\n",
    "$\\mathbf{x}$: input (shape: $I \\times 1$)\n",
    "\n",
    "$\\mathbf{z_1} = W_1\\cdot \\mathbf{x}$ (shape: $J \\times 1$)\n",
    "\n",
    "$\\mathbf{a_1} = f(\\mathbf{z_1})$ (shape: $J \\times 1$)\n",
    "\n",
    "$\\mathbf{z_2} = W_2 \\cdot \\mathbf{a_1}$ (shape: $K \\times 1$)\n",
    "\n",
    "$\\mathbf{\\hat y} = f(\\mathbf{z_2})$: output (shape: $K \\times 1$)\n",
    "\n",
    "$\\mathbf{\\delta_1} = [\\delta_j]$ (shape: $J \\times 1$)\n",
    "\n",
    "$\\mathbf{\\delta_2} = [\\delta_k]$ (shape: $K \\times 1$)\n",
    "\n",
    "Update rules:\n",
    "\n",
    "$W_1 := W_1 - \\alpha \\mathbf{\\delta_1}^T \\cdot \\mathbf{x}$ \n",
    "\n",
    "$W_2 := W_2 - \\alpha \\mathbf{\\delta_2}^T \\cdot \\mathbf{a_1}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2a)\n",
    "### Training:\n",
    "Mean: 33.55274553571429\n",
    "\n",
    "Standard deviation: 78.87550070784701\n",
    "\n",
    "\n",
    "### Validation:\n",
    "Mean: 33.791224489795916\n",
    "\n",
    "Standard deviation: 79.17246322228644"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "\n",
    "![task_2c](task2c_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "\n",
    "The number of parameteres, consisting of weights and biases, in the network are: $(28\\cdot 28 + 1)\\cdot 64 + (64 + 1) \\cdot 10 = 50890$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved weigth initialization \n",
    "![asd](task3_init.png)\n",
    "\n",
    "With improved weight initialization the accuracy and loss in both training and validation gets over 90% very fast. In addition the accuracy becomes better at the end as well. However it still took a long time to train using 42 epochs before the early stopping ended the training. \n",
    "### _Improved_ sigmoid\n",
    "![asd](task3_sigmoid.png)\n",
    "\n",
    "The improved sigmoid function resulted in very high loss and low accuracy. We assume this is because of some error in our code, most likely in the implementation of the improved sigmoid. However, we have asked for help in the lab hours, but they could not find the error either. \n",
    "### Momentum\n",
    "\n",
    "![](task3_momentum.png)\n",
    "\n",
    "When using momentum, the accuracy and loss of both the training and validation improves fast, but it seems that it is not as fast as the training with improved weight initialization in the beginning. However, it trains really fast towards the end as well stopping after only 18 epochs with very good accuracy (but not as good as with weight init). The loss and accuracy seems also to be more noisy than before. This makes sense since with momentum one is not going directly in the direction of best improvement for this specific step, but (hopefully) in the more general direction of decent of the whole error/loss surface.  \n",
    "\n",
    "### Improved weigth initialization, improved sigmoid and momentum\n",
    "![](task3_all.png)\n",
    "\n",
    "This is really bad probably because the improved sigmoid implementation does not work. Therefore we ended up just using momentum and weight initialization as shown below. This is also what we use in task 4. \n",
    "\n",
    "### Improved weigth initialization and momentum\n",
    "![](task3_momentum_and_init.png)\n",
    "\n",
    "With both improved weight initialization and momentum one can see that it results is something with \"the best of both worlds\". It improves really fast in the beginning. It seems that a combination of both is better in the beginning compared to the two improvements separately. It also improves fast towards the end as well stopping at epoch 18 similarly to when using only momentum. Also similar to when using momentum, the loss and accuracy improves with more \"noise\", but it seems that the effect is smaller now than with using only momentum. \n",
    "\n",
    "The training accuracy ends at 100% this suggests that one might be overfitting to the training set. However, the accuracy on the validation set is also very high so it does not seem that it is having a bad effect.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "With 32 nodes in the hidden layer we see that the loss and accuracy, while not being bad, have become worse than with the original 64 nodes. It converges very quickly and stop training which is good. However, this is at the cost of the accuracy decreasing with $\\approx 0.02$. The reason for the early stopping could be a result from the algorithm simply reaching its full potential, in regards to performance, which is not as good as previously given the fewer amount of nodes.\n",
    "\n",
    "![](task4a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "With 128 nodes in the hidden layer, we see the loss and accuracy improving compared to 64. It reaches the maximum accuracy and minimal loss at an earlier stage than previously, which is a result from having more nodes (i.e. less restrictions). However, the training accuracy is _suspicioulsy_ high, similar to the case with 64 nodes. This is an indication of overfitting, which should be handled by either implementing regularization, reducing number of nodes, etc.\n",
    "\n",
    "![](task4b.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "There are $(28\\cdot 28 + 1)\\cdot 64 + (64 + 1) \\cdot 10 = 50890$ parameters in with nodes per layer $[784, 64, 10]$\n",
    "\n",
    "N = number of nodes in each hidden layer\n",
    "General number of parameters with two hidden layers of size N: $(28 \\cdot 28 + 1) \\cdot N + (N + 1)\\cdot N + (N + 1) \\cdot 10 = N^2 + 796N + 10$\n",
    "\n",
    "Solving $N^2 + 796N + 10 = 50890 \\implies N \\approx 59.476$. Choosing $N = 60$ results in $60^2 + 796 * 60 + 10 = 51370$ parameters.\n",
    "\n",
    "![](task4d.png)\n",
    "\n",
    "With two hidden layers containing 60 nodes each, we see the accuracy is more noisy than before. It is also a bit lower than in the previous network. The loss is also more noisy towards the end, whereas the in task 3 (with both improved weight initialization and momentum) it stops oscillating at the end. \n",
    "\n",
    "However, it looks like there is less overfitting with this network. This is assumed from looking at how the training accuracy does not continue to improve much after the validation accuracy seem to converge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "\n",
    "![](task4e_loss.png)\n",
    "![](task4e_accuracy.png)\n",
    "\n",
    "From the plots, it looks like the network with 10 hidden layers never improves. A reason for this could be that it's too deep... The information from the output layer used in calculating the derivatives become less and less useful the further back we propagate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a280adf7d83979135d02f79e1d17d51fcc454b9a8f9cfa12ffc7a5d5efd55432"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
